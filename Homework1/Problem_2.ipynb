{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Getting familiar with TensorFlow\n",
    "\n",
    "*TensorFlow* is one of the most popular deep learning framework developed by Google. If you are new to TensorFlow, please read and play with the sample in [Getting started with TensorFlow](https://www.tensorflow.org/get_started/get_started) to get started.\n",
    "\n",
    "* <b>Learning Objective:</b> In Problem 1, you implemented a fully connected network from scratch on your own. Very tedious to do it all by yourself, right? Well, we actually feel the same thing, that's why we are using tools instead of doing everything from scratch. For this part of the assignment, we will familiarize you with a widely-used deep learning framework developed by Google, TensorFlow and walk you through convolutional neural networks and show how to train them.\n",
    "* <b>Provided Codes:</b> We provide the Template class for a simple CNN model as BaseModel, predefined skeletons for conv2d() and max_pool(), as well as the dataset preprocessing parts.\n",
    "* <b>TODOs:</b> You are asked to implement the BaseModel following the detailed instructions and design your own model in YourModel to achieve a reasonably good performance for classification task on CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 1.14.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "# Add whatever you want\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from lib.datasets import CIFAR10_tf\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# We recommend to use tensorflow==1.14.0\n",
    "print(\"TensorFlow Version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets\n",
    "Download [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz) and load the dataset. In this assignment, we will use the standard 50,000 images for training and 10,000 images for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "num_training = 49000\n",
    "num_validation = 50000 - num_training\n",
    "num_test = 10000\n",
    "\n",
    "data = CIFAR10_tf(num_training=num_training,\n",
    "                  num_validation=num_validation,\n",
    "                  num_test=num_test)\n",
    "\n",
    "# Load cifar-10 data\n",
    "X_train, Y_train = data['data_train'], data['labels_train']\n",
    "X_val, Y_val = data['data_val'], data['labels_val']\n",
    "X_test, Y_test = data['data_test'], data['labels_test']\n",
    "\n",
    "# Check the shape of the dataset\n",
    "assert X_train.shape == (num_training, 32, 32, 3)\n",
    "assert Y_train.shape == (num_training, )\n",
    "assert X_val.shape == (num_validation, 32, 32, 3)\n",
    "assert Y_val.shape == (num_validation, )\n",
    "assert X_test.shape == (num_test, 32, 32, 3)\n",
    "assert Y_test.shape == (10000, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2-1 [10pt]\n",
    "\n",
    "Using the code provided, implement a neural network architecture with an optimization routine according to the specification provided below.\n",
    "\n",
    "**Model:**\n",
    "- Input image with the size 32x32x3\n",
    "- 7x7 convolutional layer with 32 filters, stride of 1, and padding 'SAME'\n",
    "- ReLU activation layer\n",
    "- 3x3 max pooling layer with a stride of 2\n",
    "- 5x5 convolutional layer with 64 filters, stride of 1, and padding 'SAME'\n",
    "- ReLU activation layer\n",
    "- 3x3 max pooling layer with a stride of 2\n",
    "- Flatten layer (8x8x64 -> 4096)\n",
    "- Fully-connected layer with 384 output units (4096 -> 384)\n",
    "- ReLU activation layer\n",
    "- Fully-connected layer with 10 output units (384 -> 10)\n",
    "- Output logits (10)\n",
    "\n",
    "**Optimizer:**\n",
    "- Adam optimizer\n",
    "\n",
    "**Learning rate:**\n",
    "- Set start learning rate as 5e-4 and apply exponential decay every 500 steps with a base of 0.96\n",
    "- Use 'tf.train.exponential_decay' and 'tf.train.AdamOptimizer'\n",
    "\n",
    "**Loss:**\n",
    "- Softmax cross entropy loss\n",
    "- Use 'tf.nn.softmax_cross_entropy_with_logits_v2'\n",
    "\n",
    "\n",
    "Your model **should** achieve about 55% accuracy on test set in 5 epochs using provided evaluation code.\n",
    "\n",
    "You can modify the template code as you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max pooling and conv layers\n",
    "\n",
    "def conv2d(input, kernel_size, stride, num_filter):\n",
    "    stride_shape = [1, stride, stride, 1]\n",
    "    filter_shape = [kernel_size, kernel_size, input.get_shape()[3], num_filter]\n",
    "\n",
    "    W = tf.get_variable('w', filter_shape, tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "    b = tf.get_variable('b', [1, 1, 1, num_filter], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.nn.conv2d(input, W, stride_shape, padding='SAME') + b\n",
    "\n",
    "def max_pool(input, kernel_size, stride):\n",
    "    ksize = [1, kernel_size, kernel_size, 1]\n",
    "    strides = [1, stride, stride, 1]\n",
    "    return tf.nn.max_pool(input, ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Complete the following functions                                    #\n",
    "#############################################################################\n",
    "def flatten(input):\n",
    "    \"\"\"\n",
    "        - input: input tensors\n",
    "    \"\"\"\n",
    "    return tf.layers.Flatten()(input)\n",
    "\n",
    "def fc(input, num_output):\n",
    "    \"\"\"\n",
    "        - input: input tensors\n",
    "        - num_output: int, the output dimension\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.fully_connected(input, num_output,activation_fn=None)\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    def __init__(self):\n",
    "        self.num_epoch = 5\n",
    "        self.batch_size = 64\n",
    "        self.log_step = 50\n",
    "        self._build_model()\n",
    "\n",
    "    def _model(self):\n",
    "        print('-' * 5 + '  Sample model  ' + '-' * 5)\n",
    "\n",
    "        print('intput layer: ' + str(self.X.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('conv1'):\n",
    "            self.conv1 = conv2d(self.X, 7, 1, 32)\n",
    "            self.relu1 = tf.nn.relu(self.conv1)\n",
    "            self.pool1 = max_pool(self.relu1, 3, 2)            \n",
    "            print('conv1 layer: ' + str(self.pool1.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('conv2'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.conv2 = conv2d(self.pool1, 5, 1, 64)\n",
    "            self.relu2 = tf.nn.relu(self.conv2)\n",
    "            self.pool2 = max_pool(self.relu2, 3, 2)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('conv2 layer: ' + str(self.pool2.get_shape()))\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: Flatten the output tensor from conv2 layer                          #\n",
    "        #############################################################################\n",
    "        self.flat = flatten(self.pool2)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################      \n",
    "        print('flat layer: ' + str(self.flat.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('fc3'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.fc3 = fc(self.flat, 384)\n",
    "            self.relu3 = tf.nn.relu(self.fc3)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('fc3 layer: ' + str(self.relu3.get_shape()))\n",
    "            \n",
    "        with tf.variable_scope('fc4'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.fc4 = fc(self.relu3, 10)      \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('fc4 layer: ' + str(self.fc4.get_shape()))\n",
    "            \n",
    "        # Return the last layer\n",
    "        return self.fc4\n",
    "\n",
    "    def _input_ops(self):\n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: You can add any placeholders                                        #\n",
    "        #############################################################################\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        #############################################################################\n",
    "        # TODO: Adam optimizer 'self.train_op' that minimizes 'self.loss_op'        #\n",
    "        #############################################################################\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        learning_rate = tf.train.exponential_decay(5e-4,global_step,500,0.96,staircase=True)\n",
    "        self.train_op = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(self.loss_op, global_step=global_step) \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        \n",
    "    def _loss(self, labels, logits):\n",
    "        #############################################################################\n",
    "        # TODO: Softmax cross entropy loss 'self.loss_op'                           #\n",
    "        #############################################################################\n",
    "        self.loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels,logits=logits))\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Define input variables\n",
    "        self._input_ops()\n",
    "\n",
    "        # Convert Y to one-hot vector\n",
    "        labels = tf.one_hot(self.Y, 10)\n",
    "\n",
    "        # Build a model and get logits\n",
    "        logits = self._model()\n",
    "\n",
    "        # Compute loss\n",
    "        self._loss(labels, logits)\n",
    "        \n",
    "        # Build optimizer\n",
    "        self._build_optimizer()\n",
    "\n",
    "        # Compute accuracy\n",
    "        predict = tf.argmax(logits, 1)\n",
    "        correct = tf.equal(predict, self.Y)\n",
    "        self.accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "    def train(self, sess, X_train, Y_train, X_val, Y_val):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        print('-' * 5 + '  Start training  ' + '-' * 5)\n",
    "        for epoch in range(self.num_epoch):\n",
    "            print('train for epoch %d' % epoch)\n",
    "            for i in range(num_training // self.batch_size):\n",
    "                X_ = X_train[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "                Y_ = Y_train[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\n",
    "                #############################################################################\n",
    "                # TODO: You can change feed data as you want                                #\n",
    "                #############################################################################\n",
    "                feed_dict = {self.X : X_, self.Y : Y_}       \n",
    "                #############################################################################\n",
    "                #                             END OF YOUR CODE                              #\n",
    "                #############################################################################\n",
    "                fetches = [self.train_op, self.loss_op, self.accuracy_op]\n",
    "\n",
    "                _, loss, accuracy = sess.run(fetches, feed_dict=feed_dict)\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "\n",
    "                if step % self.log_step == 0:\n",
    "                    print('iteration (%d): loss = %.3f, accuracy = %.3f' %\n",
    "                        (step, loss, accuracy))\n",
    "                step += 1\n",
    "\n",
    "            # Print validation results\n",
    "            print('validation for epoch %d' % epoch)\n",
    "            val_accuracy = self.evaluate(sess, X_val, Y_val)\n",
    "            print('-  epoch %d: validation accuracy = %.3f' % (epoch, val_accuracy))\n",
    "            \n",
    "        #############################################################################\n",
    "        # TODO: Plot training curve                                                 #\n",
    "        #############################################################################\n",
    "        # Graph 1. X: iteration (training step), Y: training loss\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(losses,'b')\n",
    "        plt.title(\"Training Losses\")\n",
    "        # Graph 2. X: iteration (training step), Y: training accuracy\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(accuracies,'r')\n",
    "        plt.title(\"Training Accuracies\")\n",
    "        plt.show()\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def evaluate(self, sess, X_eval, Y_eval):\n",
    "        eval_accuracy = 0.0\n",
    "        eval_iter = 0\n",
    "        for i in range(X_eval.shape[0] // self.batch_size):\n",
    "            X_ = X_eval[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "            Y_ = Y_eval[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            \n",
    "            #############################################################################\n",
    "            # TODO: You can change feed data as you want                                #\n",
    "            #############################################################################\n",
    "            feed_dict = {self.X : X_, self.Y : Y_}  \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            accuracy = sess.run(self.accuracy_op, feed_dict=feed_dict)\n",
    "            eval_accuracy += accuracy\n",
    "            eval_iter += 1\n",
    "        return eval_accuracy / eval_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Sample model  -----\n",
      "intput layer: (?, 32, 32, 3)\n",
      "conv1 layer: (?, 16, 16, 32)\n",
      "conv2 layer: (?, 8, 8, 64)\n",
      "flat layer: (?, 4096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0926 11:57:33.636777 26684 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0926 11:57:34.193125 26684 deprecation.py:323] From <ipython-input-4-58a466b6e4b9>:81: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc3 layer: (?, 384)\n",
      "fc4 layer: (?, 10)\n",
      "-----  Start training  -----\n",
      "train for epoch 0\n",
      "iteration (0): loss = 11.839, accuracy = 0.141\n",
      "iteration (50): loss = 2.134, accuracy = 0.141\n",
      "iteration (100): loss = 1.938, accuracy = 0.297\n",
      "iteration (150): loss = 2.036, accuracy = 0.359\n",
      "iteration (200): loss = 1.589, accuracy = 0.453\n",
      "iteration (250): loss = 1.675, accuracy = 0.375\n",
      "iteration (300): loss = 1.687, accuracy = 0.312\n",
      "iteration (350): loss = 1.562, accuracy = 0.422\n",
      "iteration (400): loss = 1.476, accuracy = 0.484\n",
      "iteration (450): loss = 1.405, accuracy = 0.469\n",
      "iteration (500): loss = 1.451, accuracy = 0.578\n",
      "iteration (550): loss = 1.342, accuracy = 0.531\n",
      "iteration (600): loss = 1.566, accuracy = 0.469\n",
      "iteration (650): loss = 1.278, accuracy = 0.531\n",
      "iteration (700): loss = 1.385, accuracy = 0.469\n",
      "iteration (750): loss = 1.765, accuracy = 0.422\n",
      "validation for epoch 0\n",
      "-  epoch 0: validation accuracy = 0.521\n",
      "train for epoch 1\n",
      "iteration (800): loss = 1.372, accuracy = 0.516\n",
      "iteration (850): loss = 1.399, accuracy = 0.531\n",
      "iteration (900): loss = 1.129, accuracy = 0.578\n",
      "iteration (950): loss = 1.366, accuracy = 0.578\n",
      "iteration (1000): loss = 1.466, accuracy = 0.547\n",
      "iteration (1050): loss = 1.380, accuracy = 0.516\n",
      "iteration (1100): loss = 1.488, accuracy = 0.531\n",
      "iteration (1150): loss = 1.147, accuracy = 0.625\n",
      "iteration (1200): loss = 1.366, accuracy = 0.516\n",
      "iteration (1250): loss = 0.934, accuracy = 0.656\n",
      "iteration (1300): loss = 1.008, accuracy = 0.609\n",
      "iteration (1350): loss = 1.332, accuracy = 0.500\n",
      "iteration (1400): loss = 1.052, accuracy = 0.641\n",
      "iteration (1450): loss = 1.446, accuracy = 0.516\n",
      "iteration (1500): loss = 1.177, accuracy = 0.625\n",
      "validation for epoch 1\n",
      "-  epoch 1: validation accuracy = 0.582\n",
      "train for epoch 2\n",
      "iteration (1550): loss = 1.479, accuracy = 0.484\n",
      "iteration (1600): loss = 1.318, accuracy = 0.562\n",
      "iteration (1650): loss = 1.152, accuracy = 0.625\n",
      "iteration (1700): loss = 1.329, accuracy = 0.469\n",
      "iteration (1750): loss = 1.053, accuracy = 0.562\n",
      "iteration (1800): loss = 1.229, accuracy = 0.641\n",
      "iteration (1850): loss = 1.057, accuracy = 0.594\n",
      "iteration (1900): loss = 0.831, accuracy = 0.703\n",
      "iteration (1950): loss = 1.255, accuracy = 0.500\n",
      "iteration (2000): loss = 1.021, accuracy = 0.625\n",
      "iteration (2050): loss = 1.254, accuracy = 0.562\n",
      "iteration (2100): loss = 1.007, accuracy = 0.656\n",
      "iteration (2150): loss = 1.005, accuracy = 0.641\n",
      "iteration (2200): loss = 1.315, accuracy = 0.500\n",
      "iteration (2250): loss = 0.900, accuracy = 0.641\n",
      "validation for epoch 2\n",
      "-  epoch 2: validation accuracy = 0.621\n",
      "train for epoch 3\n",
      "iteration (2300): loss = 0.876, accuracy = 0.656\n",
      "iteration (2350): loss = 1.005, accuracy = 0.625\n",
      "iteration (2400): loss = 0.838, accuracy = 0.672\n",
      "iteration (2450): loss = 1.162, accuracy = 0.625\n",
      "iteration (2500): loss = 0.770, accuracy = 0.734\n",
      "iteration (2550): loss = 0.863, accuracy = 0.703\n",
      "iteration (2600): loss = 1.271, accuracy = 0.531\n",
      "iteration (2650): loss = 0.894, accuracy = 0.656\n",
      "iteration (2700): loss = 0.849, accuracy = 0.703\n",
      "iteration (2750): loss = 1.126, accuracy = 0.609\n",
      "iteration (2800): loss = 0.932, accuracy = 0.625\n",
      "iteration (2850): loss = 0.934, accuracy = 0.609\n",
      "iteration (2900): loss = 0.820, accuracy = 0.734\n",
      "iteration (2950): loss = 0.976, accuracy = 0.672\n",
      "iteration (3000): loss = 0.762, accuracy = 0.688\n",
      "iteration (3050): loss = 1.024, accuracy = 0.641\n",
      "validation for epoch 3\n",
      "-  epoch 3: validation accuracy = 0.626\n",
      "train for epoch 4\n",
      "iteration (3100): loss = 0.787, accuracy = 0.719\n",
      "iteration (3150): loss = 0.953, accuracy = 0.688\n",
      "iteration (3200): loss = 1.024, accuracy = 0.578\n",
      "iteration (3250): loss = 0.856, accuracy = 0.703\n",
      "iteration (3300): loss = 0.777, accuracy = 0.750\n",
      "iteration (3350): loss = 0.856, accuracy = 0.641\n",
      "iteration (3400): loss = 0.811, accuracy = 0.703\n",
      "iteration (3450): loss = 0.649, accuracy = 0.719\n",
      "iteration (3500): loss = 0.715, accuracy = 0.766\n",
      "iteration (3550): loss = 0.956, accuracy = 0.703\n",
      "iteration (3600): loss = 1.110, accuracy = 0.672\n",
      "iteration (3650): loss = 0.848, accuracy = 0.703\n",
      "iteration (3700): loss = 1.127, accuracy = 0.562\n",
      "iteration (3750): loss = 0.875, accuracy = 0.703\n",
      "iteration (3800): loss = 1.267, accuracy = 0.594\n",
      "validation for epoch 4\n",
      "-  epoch 4: validation accuracy = 0.648\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** test accuracy: 0.618\n",
      "Model saved in lib/tf_models/problem2/csci-599_sample.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Clear old computation graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Train our sample model\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    with tf.device('/cpu:0'):\n",
    "        model = BaseModel()\n",
    "        model.train(sess, X_train, Y_train, X_val, Y_val)\n",
    "        accuracy = model.evaluate(sess, X_test, Y_test)\n",
    "        print('***** test accuracy: %.3f' % accuracy)\n",
    "        saver = tf.train.Saver()\n",
    "        model_path = saver.save(sess, \"lib/tf_models/problem2/csci-599_sample.ckpt\")\n",
    "        print(\"Model saved in %s\" % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2-2 [15pt]\n",
    "\n",
    "Implement your own model. \n",
    "\n",
    "You can modify the template code as you want and you can use GPU for fast training. For GPU usage, simply change the following line of the training block:  \n",
    "from `with tf.device('/cpu:0')` to `with tf.device('/GPU:0')` and you can set your desired device number.\n",
    "\n",
    "These are the techniques that you can try:\n",
    "- Data preprocessing\n",
    "- Data augmentation\n",
    "- Batch normalization\n",
    "- Dropout\n",
    "- More convolutional layers\n",
    "- More training epochs\n",
    "- Learning rate decay\n",
    "- Any other models and techniqes\n",
    "\n",
    "The rubrics for this question is:\n",
    "* 15 points when test accuracy >= 75%\n",
    "* 10 points when test accuracy >= 70%\n",
    "* 5 points when test accuracy >= 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        super(YourModel, self).__init__()\n",
    "        self.num_epoch = 20\n",
    "\n",
    "    def _model(self):\n",
    "        print('-' * 5 + '  Your model  ' + '-' * 5)\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: Implement you own model here                                        #\n",
    "        #############################################################################\n",
    "        with tf.variable_scope('conv1'):\n",
    "            self.conv1 = conv2d(self.X, 3, 1, 32)\n",
    "            self.relu1 = tf.nn.relu(self.conv1)\n",
    "            self.pool1 = max_pool(self.relu1, 2, 2)\n",
    "            print('conv1 layer: '+ str(self.pool1.get_shape()))\n",
    "            \n",
    "            \n",
    "        with tf.variable_scope('conv2'):\n",
    "            self.conv2 = conv2d(self.pool1, 3, 1, 64)\n",
    "            self.relu2 = tf.nn.relu(self.conv2)\n",
    "            self.pool2 = max_pool(self.relu2, 2, 2)\n",
    "            print('conv2 layer: '+ str(self.pool2.get_shape()))\n",
    "            \n",
    "            \n",
    "        with tf.variable_scope('conv3'):\n",
    "            self.conv3 = conv2d(self.pool2, 3, 1, 128)\n",
    "            self.relu3 = tf.nn.relu(self.conv3)\n",
    "            self.pool3 = max_pool(self.relu3, 2, 2)\n",
    "            print('conv3 layer: '+ str(self.relu3.get_shape()))\n",
    "            \n",
    "            \n",
    "        with tf.variable_scope('conv4'):\n",
    "            self.conv4 = conv2d(self.conv3, 3, 1, 256)\n",
    "            self.relu4 = tf.nn.relu(self.conv4)\n",
    "            self.pool4 = max_pool(self.relu4, 2, 2)\n",
    "            print('conv4 layer: '+ str(self.pool4.get_shape()))\n",
    "            \n",
    "            \n",
    "        with tf.variable_scope('conv5'):\n",
    "            self.conv5 = conv2d(self.pool4, 3, 1, 512)\n",
    "            self.relu5 = tf.nn.relu(self.conv5)\n",
    "            self.pool5 = max_pool(self.relu5, 2, 2)\n",
    "            print('conv5 layer: '+ str(self.pool5.get_shape()))\n",
    "            \n",
    "        self.flat = flatten(self.pool5)\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('fc6'):\n",
    "            self.fc4 = fc(self.flat, 128)\n",
    "            print('fc4 layer: '+ str(self.fc4.get_shape()))\n",
    "            \n",
    "            \n",
    "        with tf.variable_scope('fc7'):\n",
    "            self.fc5 = fc(self.fc4, 10)\n",
    "            print('fc5 layer: '+ str(self.fc5.get_shape()))\n",
    "        return self.fc5\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Your model  -----\n",
      "conv1 layer: (?, 16, 16, 32)\n",
      "conv2 layer: (?, 8, 8, 64)\n",
      "conv3 layer: (?, 8, 8, 128)\n",
      "conv4 layer: (?, 4, 4, 256)\n",
      "conv5 layer: (?, 2, 2, 512)\n",
      "fc4 layer: (?, 128)\n",
      "fc5 layer: (?, 10)\n",
      "-----  Start training  -----\n",
      "train for epoch 0\n",
      "iteration (0): loss = 2.407, accuracy = 0.062\n",
      "iteration (50): loss = 1.877, accuracy = 0.312\n",
      "iteration (100): loss = 1.684, accuracy = 0.344\n",
      "iteration (150): loss = 1.819, accuracy = 0.344\n",
      "iteration (200): loss = 1.429, accuracy = 0.531\n",
      "iteration (250): loss = 1.378, accuracy = 0.500\n",
      "iteration (300): loss = 1.157, accuracy = 0.578\n",
      "iteration (350): loss = 1.379, accuracy = 0.453\n",
      "iteration (400): loss = 1.169, accuracy = 0.641\n",
      "iteration (450): loss = 1.133, accuracy = 0.594\n",
      "iteration (500): loss = 1.324, accuracy = 0.562\n",
      "iteration (550): loss = 0.973, accuracy = 0.625\n",
      "iteration (600): loss = 1.063, accuracy = 0.688\n",
      "iteration (650): loss = 0.911, accuracy = 0.688\n",
      "iteration (700): loss = 1.155, accuracy = 0.656\n",
      "iteration (750): loss = 1.480, accuracy = 0.609\n",
      "validation for epoch 0\n",
      "-  epoch 0: validation accuracy = 0.684\n",
      "train for epoch 1\n",
      "iteration (800): loss = 0.780, accuracy = 0.719\n",
      "iteration (850): loss = 0.960, accuracy = 0.672\n",
      "iteration (900): loss = 0.817, accuracy = 0.656\n",
      "iteration (950): loss = 0.819, accuracy = 0.672\n",
      "iteration (1000): loss = 1.076, accuracy = 0.641\n",
      "iteration (1050): loss = 1.050, accuracy = 0.609\n",
      "iteration (1100): loss = 0.969, accuracy = 0.688\n",
      "iteration (1150): loss = 0.694, accuracy = 0.781\n",
      "iteration (1200): loss = 1.100, accuracy = 0.594\n",
      "iteration (1250): loss = 0.772, accuracy = 0.734\n",
      "iteration (1300): loss = 0.587, accuracy = 0.797\n",
      "iteration (1350): loss = 1.018, accuracy = 0.656\n",
      "iteration (1400): loss = 0.567, accuracy = 0.766\n",
      "iteration (1450): loss = 0.771, accuracy = 0.734\n",
      "iteration (1500): loss = 0.719, accuracy = 0.719\n",
      "validation for epoch 1\n",
      "-  epoch 1: validation accuracy = 0.705\n",
      "train for epoch 2\n",
      "iteration (1550): loss = 0.701, accuracy = 0.766\n",
      "iteration (1600): loss = 0.918, accuracy = 0.672\n",
      "iteration (1650): loss = 0.855, accuracy = 0.672\n",
      "iteration (1700): loss = 0.975, accuracy = 0.594\n",
      "iteration (1750): loss = 0.658, accuracy = 0.688\n",
      "iteration (1800): loss = 0.713, accuracy = 0.781\n",
      "iteration (1850): loss = 0.634, accuracy = 0.766\n",
      "iteration (1900): loss = 0.372, accuracy = 0.875\n",
      "iteration (1950): loss = 0.790, accuracy = 0.688\n",
      "iteration (2000): loss = 0.690, accuracy = 0.812\n",
      "iteration (2050): loss = 0.709, accuracy = 0.781\n",
      "iteration (2100): loss = 0.536, accuracy = 0.828\n",
      "iteration (2150): loss = 0.651, accuracy = 0.797\n",
      "iteration (2200): loss = 0.539, accuracy = 0.828\n",
      "iteration (2250): loss = 0.498, accuracy = 0.812\n",
      "validation for epoch 2\n",
      "-  epoch 2: validation accuracy = 0.701\n",
      "train for epoch 3\n",
      "iteration (2300): loss = 0.548, accuracy = 0.781\n",
      "iteration (2350): loss = 0.535, accuracy = 0.797\n",
      "iteration (2400): loss = 0.483, accuracy = 0.828\n",
      "iteration (2450): loss = 0.552, accuracy = 0.828\n",
      "iteration (2500): loss = 0.315, accuracy = 0.906\n",
      "iteration (2550): loss = 0.672, accuracy = 0.734\n",
      "iteration (2600): loss = 0.488, accuracy = 0.875\n",
      "iteration (2650): loss = 0.726, accuracy = 0.719\n",
      "iteration (2700): loss = 0.478, accuracy = 0.812\n",
      "iteration (2750): loss = 0.960, accuracy = 0.688\n",
      "iteration (2800): loss = 0.405, accuracy = 0.859\n",
      "iteration (2850): loss = 0.447, accuracy = 0.844\n",
      "iteration (2900): loss = 0.476, accuracy = 0.812\n",
      "iteration (2950): loss = 0.473, accuracy = 0.781\n",
      "iteration (3000): loss = 0.383, accuracy = 0.891\n",
      "iteration (3050): loss = 0.475, accuracy = 0.797\n",
      "validation for epoch 3\n",
      "-  epoch 3: validation accuracy = 0.718\n",
      "train for epoch 4\n",
      "iteration (3100): loss = 0.364, accuracy = 0.875\n",
      "iteration (3150): loss = 0.287, accuracy = 0.875\n",
      "iteration (3200): loss = 0.676, accuracy = 0.781\n",
      "iteration (3250): loss = 0.484, accuracy = 0.828\n",
      "iteration (3300): loss = 0.253, accuracy = 0.938\n",
      "iteration (3350): loss = 0.490, accuracy = 0.844\n",
      "iteration (3400): loss = 0.271, accuracy = 0.891\n",
      "iteration (3450): loss = 0.346, accuracy = 0.875\n",
      "iteration (3500): loss = 0.351, accuracy = 0.891\n",
      "iteration (3550): loss = 0.370, accuracy = 0.875\n",
      "iteration (3600): loss = 0.425, accuracy = 0.844\n",
      "iteration (3650): loss = 0.496, accuracy = 0.812\n",
      "iteration (3700): loss = 0.257, accuracy = 0.891\n",
      "iteration (3750): loss = 0.163, accuracy = 0.906\n",
      "iteration (3800): loss = 0.206, accuracy = 0.938\n",
      "validation for epoch 4\n",
      "-  epoch 4: validation accuracy = 0.727\n",
      "train for epoch 5\n",
      "iteration (3850): loss = 0.349, accuracy = 0.891\n",
      "iteration (3900): loss = 0.413, accuracy = 0.922\n",
      "iteration (3950): loss = 0.317, accuracy = 0.891\n",
      "iteration (4000): loss = 0.271, accuracy = 0.906\n",
      "iteration (4050): loss = 0.343, accuracy = 0.875\n",
      "iteration (4100): loss = 0.295, accuracy = 0.922\n",
      "iteration (4150): loss = 0.287, accuracy = 0.906\n",
      "iteration (4200): loss = 0.174, accuracy = 0.938\n",
      "iteration (4250): loss = 0.380, accuracy = 0.859\n",
      "iteration (4300): loss = 0.241, accuracy = 0.891\n",
      "iteration (4350): loss = 0.388, accuracy = 0.859\n",
      "iteration (4400): loss = 0.270, accuracy = 0.891\n",
      "iteration (4450): loss = 0.226, accuracy = 0.906\n",
      "iteration (4500): loss = 0.430, accuracy = 0.906\n",
      "iteration (4550): loss = 0.293, accuracy = 0.891\n",
      "validation for epoch 5\n",
      "-  epoch 5: validation accuracy = 0.755\n",
      "train for epoch 6\n",
      "iteration (4600): loss = 0.208, accuracy = 0.906\n",
      "iteration (4650): loss = 0.427, accuracy = 0.844\n",
      "iteration (4700): loss = 0.185, accuracy = 0.906\n",
      "iteration (4750): loss = 0.240, accuracy = 0.906\n",
      "iteration (4800): loss = 0.151, accuracy = 0.984\n",
      "iteration (4850): loss = 0.198, accuracy = 0.938\n",
      "iteration (4900): loss = 0.373, accuracy = 0.891\n",
      "iteration (4950): loss = 0.135, accuracy = 0.969\n",
      "iteration (5000): loss = 0.366, accuracy = 0.828\n",
      "iteration (5050): loss = 0.405, accuracy = 0.859\n",
      "iteration (5100): loss = 0.373, accuracy = 0.875\n",
      "iteration (5150): loss = 0.119, accuracy = 0.938\n",
      "iteration (5200): loss = 0.083, accuracy = 0.984\n",
      "iteration (5250): loss = 0.098, accuracy = 0.969\n",
      "iteration (5300): loss = 0.187, accuracy = 0.953\n",
      "iteration (5350): loss = 0.276, accuracy = 0.906\n",
      "validation for epoch 6\n",
      "-  epoch 6: validation accuracy = 0.728\n",
      "train for epoch 7\n",
      "iteration (5400): loss = 0.310, accuracy = 0.906\n",
      "iteration (5450): loss = 0.315, accuracy = 0.891\n",
      "iteration (5500): loss = 0.138, accuracy = 0.969\n",
      "iteration (5550): loss = 0.107, accuracy = 0.969\n",
      "iteration (5600): loss = 0.346, accuracy = 0.859\n",
      "iteration (5650): loss = 0.161, accuracy = 0.922\n",
      "iteration (5700): loss = 0.184, accuracy = 0.938\n",
      "iteration (5750): loss = 0.123, accuracy = 0.984\n",
      "iteration (5800): loss = 0.129, accuracy = 0.953\n",
      "iteration (5850): loss = 0.147, accuracy = 0.969\n",
      "iteration (5900): loss = 0.210, accuracy = 0.922\n",
      "iteration (5950): loss = 0.134, accuracy = 0.938\n",
      "iteration (6000): loss = 0.181, accuracy = 0.938\n",
      "iteration (6050): loss = 0.257, accuracy = 0.906\n",
      "iteration (6100): loss = 0.047, accuracy = 0.984\n",
      "validation for epoch 7\n",
      "-  epoch 7: validation accuracy = 0.727\n",
      "train for epoch 8\n",
      "iteration (6150): loss = 0.188, accuracy = 0.938\n",
      "iteration (6200): loss = 0.342, accuracy = 0.875\n",
      "iteration (6250): loss = 0.157, accuracy = 0.938\n",
      "iteration (6300): loss = 0.123, accuracy = 0.969\n",
      "iteration (6350): loss = 0.053, accuracy = 0.984\n",
      "iteration (6400): loss = 0.085, accuracy = 0.969\n",
      "iteration (6450): loss = 0.141, accuracy = 0.953\n",
      "iteration (6500): loss = 0.226, accuracy = 0.922\n",
      "iteration (6550): loss = 0.162, accuracy = 0.953\n",
      "iteration (6600): loss = 0.073, accuracy = 0.953\n",
      "iteration (6650): loss = 0.074, accuracy = 0.984\n",
      "iteration (6700): loss = 0.090, accuracy = 0.969\n",
      "iteration (6750): loss = 0.146, accuracy = 0.922\n",
      "iteration (6800): loss = 0.140, accuracy = 0.938\n",
      "iteration (6850): loss = 0.080, accuracy = 0.969\n",
      "validation for epoch 8\n",
      "-  epoch 8: validation accuracy = 0.752\n",
      "train for epoch 9\n",
      "iteration (6900): loss = 0.115, accuracy = 0.969\n",
      "iteration (6950): loss = 0.206, accuracy = 0.938\n",
      "iteration (7000): loss = 0.116, accuracy = 0.953\n",
      "iteration (7050): loss = 0.049, accuracy = 0.984\n",
      "iteration (7100): loss = 0.055, accuracy = 0.984\n",
      "iteration (7150): loss = 0.088, accuracy = 0.969\n",
      "iteration (7200): loss = 0.106, accuracy = 0.953\n",
      "iteration (7250): loss = 0.132, accuracy = 0.938\n",
      "iteration (7300): loss = 0.048, accuracy = 0.969\n",
      "iteration (7350): loss = 0.052, accuracy = 0.984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration (7400): loss = 0.026, accuracy = 1.000\n",
      "iteration (7450): loss = 0.062, accuracy = 0.984\n",
      "iteration (7500): loss = 0.042, accuracy = 0.984\n",
      "iteration (7550): loss = 0.104, accuracy = 0.953\n",
      "iteration (7600): loss = 0.108, accuracy = 0.969\n",
      "validation for epoch 9\n",
      "-  epoch 9: validation accuracy = 0.760\n",
      "train for epoch 10\n",
      "iteration (7650): loss = 0.192, accuracy = 0.953\n",
      "iteration (7700): loss = 0.042, accuracy = 0.969\n",
      "iteration (7750): loss = 0.121, accuracy = 0.984\n",
      "iteration (7800): loss = 0.066, accuracy = 0.984\n",
      "iteration (7850): loss = 0.013, accuracy = 1.000\n",
      "iteration (7900): loss = 0.023, accuracy = 0.984\n",
      "iteration (7950): loss = 0.080, accuracy = 0.984\n",
      "iteration (8000): loss = 0.054, accuracy = 0.969\n",
      "iteration (8050): loss = 0.079, accuracy = 0.938\n",
      "iteration (8100): loss = 0.191, accuracy = 0.938\n",
      "iteration (8150): loss = 0.034, accuracy = 1.000\n",
      "iteration (8200): loss = 0.099, accuracy = 0.969\n",
      "iteration (8250): loss = 0.007, accuracy = 1.000\n",
      "iteration (8300): loss = 0.099, accuracy = 0.984\n",
      "iteration (8350): loss = 0.145, accuracy = 0.953\n",
      "iteration (8400): loss = 0.288, accuracy = 0.922\n",
      "validation for epoch 10\n",
      "-  epoch 10: validation accuracy = 0.759\n",
      "train for epoch 11\n",
      "iteration (8450): loss = 0.070, accuracy = 0.969\n",
      "iteration (8500): loss = 0.106, accuracy = 0.953\n",
      "iteration (8550): loss = 0.167, accuracy = 0.922\n",
      "iteration (8600): loss = 0.101, accuracy = 0.969\n",
      "iteration (8650): loss = 0.069, accuracy = 0.969\n",
      "iteration (8700): loss = 0.042, accuracy = 0.984\n",
      "iteration (8750): loss = 0.030, accuracy = 0.984\n",
      "iteration (8800): loss = 0.081, accuracy = 0.953\n",
      "iteration (8850): loss = 0.152, accuracy = 0.938\n",
      "iteration (8900): loss = 0.085, accuracy = 0.969\n",
      "iteration (8950): loss = 0.073, accuracy = 0.969\n",
      "iteration (9000): loss = 0.228, accuracy = 0.906\n",
      "iteration (9050): loss = 0.130, accuracy = 0.938\n",
      "iteration (9100): loss = 0.095, accuracy = 0.953\n",
      "iteration (9150): loss = 0.094, accuracy = 0.984\n",
      "validation for epoch 11\n",
      "-  epoch 11: validation accuracy = 0.753\n",
      "train for epoch 12\n",
      "iteration (9200): loss = 0.025, accuracy = 1.000\n",
      "iteration (9250): loss = 0.128, accuracy = 0.938\n",
      "iteration (9300): loss = 0.034, accuracy = 0.984\n",
      "iteration (9350): loss = 0.127, accuracy = 0.953\n",
      "iteration (9400): loss = 0.100, accuracy = 0.953\n",
      "iteration (9450): loss = 0.033, accuracy = 0.984\n",
      "iteration (9500): loss = 0.035, accuracy = 0.984\n",
      "iteration (9550): loss = 0.004, accuracy = 1.000\n",
      "iteration (9600): loss = 0.058, accuracy = 0.984\n",
      "iteration (9650): loss = 0.059, accuracy = 0.953\n",
      "iteration (9700): loss = 0.280, accuracy = 0.938\n",
      "iteration (9750): loss = 0.021, accuracy = 1.000\n",
      "iteration (9800): loss = 0.009, accuracy = 1.000\n",
      "iteration (9850): loss = 0.030, accuracy = 0.984\n",
      "iteration (9900): loss = 0.036, accuracy = 0.984\n",
      "validation for epoch 12\n",
      "-  epoch 12: validation accuracy = 0.752\n",
      "train for epoch 13\n",
      "iteration (9950): loss = 0.096, accuracy = 0.969\n",
      "iteration (10000): loss = 0.022, accuracy = 1.000\n",
      "iteration (10050): loss = 0.004, accuracy = 1.000\n",
      "iteration (10100): loss = 0.147, accuracy = 0.984\n",
      "iteration (10150): loss = 0.054, accuracy = 0.984\n",
      "iteration (10200): loss = 0.010, accuracy = 1.000\n",
      "iteration (10250): loss = 0.031, accuracy = 0.984\n",
      "iteration (10300): loss = 0.065, accuracy = 0.984\n",
      "iteration (10350): loss = 0.031, accuracy = 1.000\n",
      "iteration (10400): loss = 0.070, accuracy = 0.953\n",
      "iteration (10450): loss = 0.005, accuracy = 1.000\n",
      "iteration (10500): loss = 0.048, accuracy = 0.984\n",
      "iteration (10550): loss = 0.127, accuracy = 0.969\n",
      "iteration (10600): loss = 0.026, accuracy = 1.000\n",
      "iteration (10650): loss = 0.057, accuracy = 0.969\n",
      "iteration (10700): loss = 0.017, accuracy = 1.000\n",
      "validation for epoch 13\n",
      "-  epoch 13: validation accuracy = 0.754\n",
      "train for epoch 14\n",
      "iteration (10750): loss = 0.029, accuracy = 0.984\n",
      "iteration (10800): loss = 0.011, accuracy = 1.000\n",
      "iteration (10850): loss = 0.189, accuracy = 0.969\n",
      "iteration (10900): loss = 0.059, accuracy = 0.969\n",
      "iteration (10950): loss = 0.045, accuracy = 0.984\n",
      "iteration (11000): loss = 0.148, accuracy = 0.969\n",
      "iteration (11050): loss = 0.138, accuracy = 0.969\n",
      "iteration (11100): loss = 0.045, accuracy = 0.969\n",
      "iteration (11150): loss = 0.045, accuracy = 0.969\n",
      "iteration (11200): loss = 0.049, accuracy = 0.969\n",
      "iteration (11250): loss = 0.002, accuracy = 1.000\n",
      "iteration (11300): loss = 0.015, accuracy = 1.000\n",
      "iteration (11350): loss = 0.011, accuracy = 1.000\n",
      "iteration (11400): loss = 0.063, accuracy = 0.984\n",
      "iteration (11450): loss = 0.010, accuracy = 1.000\n",
      "validation for epoch 14\n",
      "-  epoch 14: validation accuracy = 0.748\n",
      "train for epoch 15\n",
      "iteration (11500): loss = 0.080, accuracy = 0.969\n",
      "iteration (11550): loss = 0.013, accuracy = 1.000\n",
      "iteration (11600): loss = 0.003, accuracy = 1.000\n",
      "iteration (11650): loss = 0.018, accuracy = 0.984\n",
      "iteration (11700): loss = 0.046, accuracy = 0.984\n",
      "iteration (11750): loss = 0.048, accuracy = 0.984\n",
      "iteration (11800): loss = 0.005, accuracy = 1.000\n",
      "iteration (11850): loss = 0.013, accuracy = 1.000\n",
      "iteration (11900): loss = 0.075, accuracy = 0.984\n",
      "iteration (11950): loss = 0.075, accuracy = 0.969\n",
      "iteration (12000): loss = 0.014, accuracy = 1.000\n",
      "iteration (12050): loss = 0.009, accuracy = 1.000\n",
      "iteration (12100): loss = 0.002, accuracy = 1.000\n",
      "iteration (12150): loss = 0.012, accuracy = 1.000\n",
      "iteration (12200): loss = 0.050, accuracy = 0.969\n",
      "validation for epoch 15\n",
      "-  epoch 15: validation accuracy = 0.741\n",
      "train for epoch 16\n",
      "iteration (12250): loss = 0.073, accuracy = 0.969\n",
      "iteration (12300): loss = 0.064, accuracy = 0.984\n",
      "iteration (12350): loss = 0.002, accuracy = 1.000\n",
      "iteration (12400): loss = 0.022, accuracy = 0.984\n",
      "iteration (12450): loss = 0.017, accuracy = 0.984\n",
      "iteration (12500): loss = 0.114, accuracy = 0.969\n",
      "iteration (12550): loss = 0.002, accuracy = 1.000\n",
      "iteration (12600): loss = 0.022, accuracy = 0.984\n",
      "iteration (12650): loss = 0.006, accuracy = 1.000\n",
      "iteration (12700): loss = 0.016, accuracy = 0.984\n",
      "iteration (12750): loss = 0.018, accuracy = 0.984\n",
      "iteration (12800): loss = 0.011, accuracy = 1.000\n",
      "iteration (12850): loss = 0.002, accuracy = 1.000\n",
      "iteration (12900): loss = 0.006, accuracy = 1.000\n",
      "iteration (12950): loss = 0.037, accuracy = 0.984\n",
      "iteration (13000): loss = 0.039, accuracy = 0.984\n",
      "validation for epoch 16\n",
      "-  epoch 16: validation accuracy = 0.767\n",
      "train for epoch 17\n",
      "iteration (13050): loss = 0.003, accuracy = 1.000\n",
      "iteration (13100): loss = 0.001, accuracy = 1.000\n",
      "iteration (13150): loss = 0.014, accuracy = 1.000\n",
      "iteration (13200): loss = 0.003, accuracy = 1.000\n",
      "iteration (13250): loss = 0.061, accuracy = 0.969\n",
      "iteration (13300): loss = 0.011, accuracy = 1.000\n",
      "iteration (13350): loss = 0.003, accuracy = 1.000\n",
      "iteration (13400): loss = 0.006, accuracy = 1.000\n",
      "iteration (13450): loss = 0.037, accuracy = 1.000\n",
      "iteration (13500): loss = 0.128, accuracy = 0.953\n",
      "iteration (13550): loss = 0.002, accuracy = 1.000\n",
      "iteration (13600): loss = 0.046, accuracy = 0.969\n",
      "iteration (13650): loss = 0.004, accuracy = 1.000\n",
      "iteration (13700): loss = 0.002, accuracy = 1.000\n",
      "iteration (13750): loss = 0.008, accuracy = 1.000\n",
      "validation for epoch 17\n",
      "-  epoch 17: validation accuracy = 0.741\n",
      "train for epoch 18\n",
      "iteration (13800): loss = 0.005, accuracy = 1.000\n",
      "iteration (13850): loss = 0.006, accuracy = 1.000\n",
      "iteration (13900): loss = 0.002, accuracy = 1.000\n",
      "iteration (13950): loss = 0.002, accuracy = 1.000\n",
      "iteration (14000): loss = 0.005, accuracy = 1.000\n",
      "iteration (14050): loss = 0.002, accuracy = 1.000\n",
      "iteration (14100): loss = 0.002, accuracy = 1.000\n",
      "iteration (14150): loss = 0.036, accuracy = 0.984\n",
      "iteration (14200): loss = 0.001, accuracy = 1.000\n",
      "iteration (14250): loss = 0.016, accuracy = 0.984\n",
      "iteration (14300): loss = 0.012, accuracy = 1.000\n",
      "iteration (14350): loss = 0.001, accuracy = 1.000\n",
      "iteration (14400): loss = 0.007, accuracy = 1.000\n",
      "iteration (14450): loss = 0.012, accuracy = 0.984\n",
      "iteration (14500): loss = 0.005, accuracy = 1.000\n",
      "validation for epoch 18\n",
      "-  epoch 18: validation accuracy = 0.761\n",
      "train for epoch 19\n",
      "iteration (14550): loss = 0.006, accuracy = 1.000\n",
      "iteration (14600): loss = 0.009, accuracy = 1.000\n",
      "iteration (14650): loss = 0.011, accuracy = 1.000\n",
      "iteration (14700): loss = 0.001, accuracy = 1.000\n",
      "iteration (14750): loss = 0.102, accuracy = 0.984\n",
      "iteration (14800): loss = 0.006, accuracy = 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration (14850): loss = 0.102, accuracy = 0.984\n",
      "iteration (14900): loss = 0.007, accuracy = 1.000\n",
      "iteration (14950): loss = 0.001, accuracy = 1.000\n",
      "iteration (15000): loss = 0.009, accuracy = 1.000\n",
      "iteration (15050): loss = 0.025, accuracy = 0.984\n",
      "iteration (15100): loss = 0.004, accuracy = 1.000\n",
      "iteration (15150): loss = 0.008, accuracy = 1.000\n",
      "iteration (15200): loss = 0.008, accuracy = 1.000\n",
      "iteration (15250): loss = 0.002, accuracy = 1.000\n",
      "validation for epoch 19\n",
      "-  epoch 19: validation accuracy = 0.758\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** test accuracy: 0.743\n",
      "Model saved in lib/tf_models/problem2/csci-599_mine.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Clear old computation graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    with tf.device('/GPU:0'):\n",
    "        model = YourModel()\n",
    "        model.train(sess, X_train, Y_train, X_val, Y_val)\n",
    "        accuracy = model.evaluate(sess, X_test, Y_test)\n",
    "        print('***** test accuracy: %.3f' % accuracy)\n",
    "        # Save your model\n",
    "        saver = tf.train.Saver()\n",
    "        model_path = saver.save(sess, \"lib/tf_models/problem2/csci-599_mine.ckpt\")\n",
    "        print(\"Model saved in %s\" % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Your model  -----\n",
      "conv1 layer: (?, 16, 16, 32)\n",
      "conv2 layer: (?, 8, 8, 64)\n",
      "conv3 layer: (?, 8, 8, 128)\n",
      "conv4 layer: (?, 4, 4, 256)\n",
      "conv5 layer: (?, 2, 2, 512)\n",
      "fc4 layer: (?, 128)\n",
      "fc5 layer: (?, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0926 13:44:58.418786 26684 deprecation.py:323] From C:\\Users\\11490\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Load your model\n",
    "model = YourModel()\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"lib/tf_models/problem2/csci-599_mine.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
